
# ğŸ”¥ A/B + Ensemble Project: Predicting Calorie Expenditure

This project tackles a regression problem from the **Kaggle Playground Series (S5E5)** competition. The goal is to accurately predict how many calories a person burns during physical activity based on biometric and workout-related features.

---

## ğŸ¯ Objective

To develop and optimize models that predict calorie expenditure using a combination of:

- Biometric features (age, height, weight, gender)
- Activity features (duration, heart rate, body temperature)

---

## ğŸ“Š Methodology

### 1. Data Cleaning & Preparation

- Dropped unused or redundant columns
- Encoded categorical variables (e.g., gender)
- Handled outliers and verified unit consistency
- Engineered key features:
  - BMI = Weight / HeightÂ²
  - Interaction terms (e.g., Duration Ã— Heart Rate)
  - Polynomial features (DurationÂ²)

---

### 2. Model Selection & Training

- Chose five powerful regression models:
  - `RandomForestRegressor`
  - `HistGradientBoostingRegressor`
  - `XGBRegressor`
  - `LGBMRegressor`
  - `CatBoostRegressor`

- Each model was:
  - Tuned using **Optuna** (100 trials/model)
  - Evaluated using **RMSLE** with 5-fold CV
  - Tracked with **MLflow** for reproducibility

---

### 3. Ensemble Strategy

- **Optuna-weighted averaging** of 5 base models:
  - Weights were optimized to minimize RMSLE
  - Final prediction formula:
    ```
    final_pred = w1 * RF + w2 * HGB + w3 * XGB + w4 * LGBM + w5 * CatBoost
    ```

- Submission using this ensemble achieved:
  âœ… **Leaderboard RMSLE: 0.05798** (Top 9%)

---

### 4. Stacking & OOF (Advanced)

- Implemented **Out-of-Fold (OOF) stacking**:
  - Generated OOF predictions for each base model
  - Trained a **Ridge meta-model** on these as inputs
  - Produced a stacked test prediction

- Also explored:
  - Manual weight ensembles
  - Ridge + Ensemble blending
  - Weighted correlation-based stacking

---

## ğŸ“ˆ Results

| Model        | CV RMSLE (Validation) | Leaderboard RMSLE |
|--------------|-----------------------|--------------------|
| RandomForest | 0.063xx               | â€”                  |
| XGBoost      | 0.058xx               | â€”                  |
| CatBoost     | 0.057xx               | â€”                  |
| Ensemble     | 0.05750               | âœ… **0.05798**      |

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ processed
â”‚Â Â  â””â”€â”€ raw
â”œâ”€â”€ logs
â”‚Â Â  â”œâ”€â”€ best_params
â”‚Â Â  â”œâ”€â”€ best_params_orig_features
â”‚Â Â  â”œâ”€â”€ catboost_logs
â”‚Â Â  â””â”€â”€ mlruns
â”œâ”€â”€ models
â”œâ”€â”€ notebooks
â”‚Â Â  â”œâ”€â”€ 01_eda.ipynb
â”‚Â Â  â”œâ”€â”€ 02_baseline_model.ipynb
â”‚Â Â  â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚Â Â  â”œâ”€â”€ 04_baseline_model_fe.ipynb
â”‚Â Â  â”œâ”€â”€ 05_feature_selection.ipynb
â”‚Â Â  â”œâ”€â”€ 06_hyperparameter_tuning.ipynb
â”‚Â Â  â”œâ”€â”€ 07_ensemble_model.ipynb
â”‚Â Â  â”œâ”€â”€ init.py
â”‚Â Â  â””â”€â”€ __pycache__
â”œâ”€â”€ outputs
â”‚Â Â  â”œâ”€â”€ metrics.csv
â”‚Â Â  â”œâ”€â”€ submission_avg_ensemble.csv
â”‚Â Â  â”œâ”€â”€ submission_baseline_catboost.csv
â”‚Â Â  â”œâ”€â”€ submission_baseline_catboost_fe.csv
â”‚Â Â  â”œâ”€â”€ submission_baseline_catboost_fe_updated.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_lgbm_xgb.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_optuna_v11.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_tuned_v10.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_tuned_v3.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_tuned_v4.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_tuned_v5.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_tuned_v6.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_tuned_v7.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_tuned_v8.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_tuned_v9.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_v1.csv
â”‚Â Â  â”œâ”€â”€ submission_ensemble_v2.csv
â”‚Â Â  â”œâ”€â”€ submission_lgbm_cv.csv
â”‚Â Â  â”œâ”€â”€ submission_lgbm_v1.csv
â”‚Â Â  â”œâ”€â”€ submission_oof_stack_ridge_v1.csv
â”‚Â Â  â”œâ”€â”€ submission_stacked_ensemble.csv
â”‚Â Â  â”œâ”€â”€ submission_tuned_weighted_ensemble.csv
â”‚Â Â  â”œâ”€â”€ submission_v1.csv
â”‚Â Â  â””â”€â”€ submission_weighted_ensemble.csv
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ src
    â”œâ”€â”€ config.py
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ __pycache__
    â””â”€â”€ utils.py

```

---

## ğŸ’¡ Next Steps

- Tune **Ridge alpha** and add to ensemble stack
- Visualize SHAP values for model interpretation
- Try greedy/hill climbing ensembles
- Package code into a CLI or deploy via Streamlit

---

## ğŸ› ï¸ Tech Stack

Python, Pandas, NumPy, Scikit-learn, XGBoost, LightGBM, CatBoost, Optuna, MLflow, Matplotlib, Seaborn, Jupyter Notebook, Git, GitHub

---

## ğŸ™‹â€â™‚ï¸ Author

**Saayed Alam**  
ğŸ“Œ [GitHub](https://github.com/saayedalam) | [Kaggle](https://www.kaggle.com/saayedalam) | [LinkedIn](https://www.linkedin.com/in/saayedalam)
