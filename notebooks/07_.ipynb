{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6cf9ed7-3ac1-4756-8f5b-9477c9482e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Stacking: Use base model predictions as meta features\n",
    "train_meta = np.column_stack((val_rf, val_hgb, val_xgb, val_lgbm, val_cat))\n",
    "meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "# Train meta-model\n",
    "meta_model.fit(train_meta, y_val)\n",
    "\n",
    "# === Predict on test set with each base model\n",
    "test_rf = rf.predict(X_test)\n",
    "test_hgb = hgb.predict(X_test)\n",
    "test_xgb = xgb.predict(X_test)\n",
    "test_lgbm = lgbm.predict(X_test)\n",
    "test_cat = cat.predict(X_test)\n",
    "\n",
    "# === Meta test input and final stacked prediction\n",
    "test_meta = np.column_stack((test_rf, test_hgb, test_xgb, test_lgbm, test_cat))\n",
    "final_pred = meta_model.predict(test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04e0bb77-a964-42c7-815c-818040f74c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Running grid search for weighted ensemble...\n",
      "\n",
      "âœ… New best RMSLE: 0.061352 with weights: XGB=0.1, CAT=0.1, RF=0.1, LGBM=0.2, HGB=0.5\n",
      "âœ… New best RMSLE: 0.061016 with weights: XGB=0.1, CAT=0.1, RF=0.1, LGBM=0.3, HGB=0.4\n",
      "âœ… New best RMSLE: 0.060783 with weights: XGB=0.1, CAT=0.1, RF=0.1, LGBM=0.4, HGB=0.3\n",
      "âœ… New best RMSLE: 0.060656 with weights: XGB=0.1, CAT=0.1, RF=0.1, LGBM=0.5, HGB=0.2\n",
      "âœ… New best RMSLE: 0.060516 with weights: XGB=0.1, CAT=0.1, RF=0.2, LGBM=0.3, HGB=0.3\n",
      "âœ… New best RMSLE: 0.060366 with weights: XGB=0.1, CAT=0.1, RF=0.2, LGBM=0.4, HGB=0.2\n",
      "âœ… New best RMSLE: 0.060323 with weights: XGB=0.1, CAT=0.1, RF=0.2, LGBM=0.5, HGB=0.1\n",
      "âœ… New best RMSLE: 0.060190 with weights: XGB=0.1, CAT=0.1, RF=0.3, LGBM=0.3, HGB=0.2\n",
      "âœ… New best RMSLE: 0.060124 with weights: XGB=0.1, CAT=0.1, RF=0.3, LGBM=0.4, HGB=0.1\n",
      "âœ… New best RMSLE: 0.060038 with weights: XGB=0.1, CAT=0.1, RF=0.4, LGBM=0.3, HGB=0.1\n",
      "âœ… New best RMSLE: 0.059983 with weights: XGB=0.1, CAT=0.2, RF=0.3, LGBM=0.3, HGB=0.1\n",
      "âœ… New best RMSLE: 0.059932 with weights: XGB=0.1, CAT=0.2, RF=0.4, LGBM=0.2, HGB=0.1\n",
      "âœ… New best RMSLE: 0.059916 with weights: XGB=0.1, CAT=0.3, RF=0.3, LGBM=0.2, HGB=0.1\n",
      "âœ… New best RMSLE: 0.059901 with weights: XGB=0.1, CAT=0.3, RF=0.4, LGBM=0.1, HGB=0.1\n",
      "âœ… New best RMSLE: 0.059896 with weights: XGB=0.2, CAT=0.3, RF=0.3, LGBM=0.1, HGB=0.1\n",
      "\n",
      "ðŸŽ¯ Best weights found: (0.2, 0.3, 0.3, 0.1, 0.1) â†’ RMSLE: 0.059896\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define possible weight values (must sum to 1)\n",
    "weight_range = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "best_score = float('inf')\n",
    "best_weights = None\n",
    "\n",
    "print(\"ðŸ” Running grid search for weighted ensemble...\\n\")\n",
    "\n",
    "for w in product(weight_range, repeat=5):\n",
    "    if abs(sum(w) - 1.0) > 0.001:\n",
    "        continue  # Skip if weights don't sum to ~1\n",
    "\n",
    "    w_xgb, w_cat, w_rf, w_lgbm, w_hgb = w\n",
    "\n",
    "    val_pred = (\n",
    "        w_xgb * val_xgb +\n",
    "        w_cat * val_cat +\n",
    "        w_rf * val_rf +\n",
    "        w_lgbm * val_lgbm +\n",
    "        w_hgb * val_hgb\n",
    "    )\n",
    "\n",
    "    score = rmsle(y_val, val_pred)\n",
    "\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_weights = w\n",
    "\n",
    "        print(f\"âœ… New best RMSLE: {score:.6f} with weights: XGB={w_xgb}, CAT={w_cat}, RF={w_rf}, LGBM={w_lgbm}, HGB={w_hgb}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Best weights found: {best_weights} â†’ RMSLE: {best_score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b6403b9-8fd2-4dec-b838-f37daead3761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Best normalized weights found: {'w_xgb': 0.17485098447315206, 'w_cat': 0.3358348494844964, 'w_rf': 0.3210744149301142, 'w_lgbm': 0.16696494966768965, 'w_hgb': 0.0012748014445478604}\n",
      "ðŸ“‰ Best RMSLE: 0.05983711873380113\n",
      "âœ… Submission saved as '../outputs/submission_ensemble_optuna_v11.csv'\n"
     ]
    }
   ],
   "source": [
    "# === Define Optuna objective ===\n",
    "def weight_objective(trial):\n",
    "    # Suggest weights for each model between 0 and 1\n",
    "    w_xgb = trial.suggest_float(\"w_xgb\", 0, 1)\n",
    "    w_cat = trial.suggest_float(\"w_cat\", 0, 1)\n",
    "    w_rf = trial.suggest_float(\"w_rf\", 0, 1)\n",
    "    w_lgbm = trial.suggest_float(\"w_lgbm\", 0, 1)\n",
    "    w_hgb = trial.suggest_float(\"w_hgb\", 0, 1)\n",
    "\n",
    "    # Normalize weights to sum to 1\n",
    "    total = w_xgb + w_cat + w_rf + w_lgbm + w_hgb\n",
    "    w_xgb /= total\n",
    "    w_cat /= total\n",
    "    w_rf /= total\n",
    "    w_lgbm /= total\n",
    "    w_hgb /= total\n",
    "\n",
    "    # Create blended prediction\n",
    "    val_pred = (\n",
    "        w_xgb * val_xgb +\n",
    "        w_cat * val_cat +\n",
    "        w_rf * val_rf +\n",
    "        w_lgbm * val_lgbm +\n",
    "        w_hgb * val_hgb\n",
    "    )\n",
    "\n",
    "    # Calculate RMSLE\n",
    "    return rmsle(y_val, val_pred)\n",
    "\n",
    "# === Run Optuna ===\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(weight_objective, n_trials=200)\n",
    "\n",
    "# === Get best weights ===\n",
    "best_weights = study.best_params\n",
    "total = sum(best_weights.values())\n",
    "normalized_weights = {k: v / total for k, v in best_weights.items()}\n",
    "print(\"\\nðŸŽ¯ Best normalized weights found:\", normalized_weights)\n",
    "print(\"ðŸ“‰ Best RMSLE:\", study.best_value)\n",
    "\n",
    "# === Apply weights to test set ===\n",
    "test_weighted = (\n",
    "    normalized_weights['w_xgb'] * test_xgb +\n",
    "    normalized_weights['w_cat'] * test_cat +\n",
    "    normalized_weights['w_rf'] * test_rf +\n",
    "    normalized_weights['w_lgbm'] * test_lgbm +\n",
    "    normalized_weights['w_hgb'] * test_hgb\n",
    ")\n",
    "\n",
    "# === Save submission ===\n",
    "sample['Calories'] = test_weighted\n",
    "sample.to_csv('../outputs/submission_ensemble_optuna_v11.csv', index=False)\n",
    "print(\"âœ… Submission saved as '../outputs/submission_ensemble_optuna_v11.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c93cb4fc-efe7-4b1a-aa33-cf546213c6c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Fold 1/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1670\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 88.298465\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "ðŸ” Fold 2/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1673\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 88.289920\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "ðŸ” Fold 3/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1670\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 88.240015\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "ðŸ” Fold 4/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1671\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 88.242355\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "ðŸ” Fold 5/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1668\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 88.343152\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "âœ… OOF stacking submission saved as '../outputs/submission_oof_stack_ridge_v1.csv'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# --- Base models with best Optuna params ---\n",
    "# Assumes you've already saved: best_xgb_params, best_cat_params, etc.\n",
    "# Load best params from previously tuned Optuna studies\n",
    "xgb_params = xgb_study.best_params\n",
    "cat_params = cat_study.best_params\n",
    "rf_params = rf_study.best_params\n",
    "lgbm_params = lgbm_study.best_params\n",
    "hgb_params = hgb_study.best_params\n",
    "\n",
    "\n",
    "# === Prepare OOF containers ===\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = {\n",
    "    'xgb': np.zeros(len(X)),\n",
    "    'cat': np.zeros(len(X)),\n",
    "    'rf': np.zeros(len(X)),\n",
    "    'lgbm': np.zeros(len(X)),\n",
    "    'hgb': np.zeros(len(X))\n",
    "}\n",
    "test_preds = {\n",
    "    'xgb': np.zeros(len(X_test)),\n",
    "    'cat': np.zeros(len(X_test)),\n",
    "    'rf': np.zeros(len(X_test)),\n",
    "    'lgbm': np.zeros(len(X_test)),\n",
    "    'hgb': np.zeros(len(X_test))\n",
    "}\n",
    "\n",
    "# === Loop through folds ===\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"ðŸ” Fold {fold+1}/{n_folds}\")\n",
    "    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val_fold = X.iloc[val_idx]\n",
    "\n",
    "    # --- Train base models ---\n",
    "    xgb = XGBRegressor(**xgb_params, random_state=42, verbosity=0)\n",
    "    cat = CatBoostRegressor(**cat_params, random_state=42, verbose=0)\n",
    "    rf = RandomForestRegressor(**rf_params, random_state=42, n_jobs=-1)\n",
    "    lgbm = LGBMRegressor(**lgbm_params, random_state=42)\n",
    "    hgb = HistGradientBoostingRegressor(**hgb_params, random_state=42)\n",
    "\n",
    "    xgb.fit(X_tr, y.iloc[train_idx])\n",
    "    cat.fit(X_tr, y.iloc[train_idx])\n",
    "    rf.fit(X_tr, y.iloc[train_idx])\n",
    "    lgbm.fit(X_tr, y.iloc[train_idx])\n",
    "    hgb.fit(X_tr, y.iloc[train_idx])\n",
    "\n",
    "    # --- OOF predictions ---\n",
    "    oof_preds['xgb'][val_idx] = xgb.predict(X_val_fold)\n",
    "    oof_preds['cat'][val_idx] = cat.predict(X_val_fold)\n",
    "    oof_preds['rf'][val_idx] = rf.predict(X_val_fold)\n",
    "    oof_preds['lgbm'][val_idx] = lgbm.predict(X_val_fold)\n",
    "    oof_preds['hgb'][val_idx] = hgb.predict(X_val_fold)\n",
    "\n",
    "    # --- Accumulate test set predictions (for final stacking) ---\n",
    "    test_preds['xgb'] += xgb.predict(X_test) / n_folds\n",
    "    test_preds['cat'] += cat.predict(X_test) / n_folds\n",
    "    test_preds['rf']  += rf.predict(X_test) / n_folds\n",
    "    test_preds['lgbm'] += lgbm.predict(X_test) / n_folds\n",
    "    test_preds['hgb'] += hgb.predict(X_test) / n_folds\n",
    "\n",
    "# === Stack OOF predictions for meta-model training ===\n",
    "train_meta = np.column_stack((\n",
    "    oof_preds['xgb'],\n",
    "    oof_preds['cat'],\n",
    "    oof_preds['rf'],\n",
    "    oof_preds['lgbm'],\n",
    "    oof_preds['hgb']\n",
    "))\n",
    "\n",
    "test_meta = np.column_stack((\n",
    "    test_preds['xgb'],\n",
    "    test_preds['cat'],\n",
    "    test_preds['rf'],\n",
    "    test_preds['lgbm'],\n",
    "    test_preds['hgb']\n",
    "))\n",
    "\n",
    "# === Train Ridge meta-model ===\n",
    "meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "meta_model.fit(train_meta, y)\n",
    "\n",
    "# === Final stacked prediction ===\n",
    "final_pred = meta_model.predict(test_meta)\n",
    "\n",
    "# === Save submission ===\n",
    "sample['Calories'] = final_pred\n",
    "sample.to_csv('../outputs/submission_oof_stack_ridge_v1.csv', index=False)\n",
    "print(\"âœ… OOF stacking submission saved as '../outputs/submission_oof_stack_ridge_v1.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d4297c6-ae44-47d8-9eed-15f4f435361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Submission file saved as '../outputs/submission_ensemble_tuned_v9.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save submission\n",
    "sample['Calories'] = final_pred\n",
    "sample.to_csv('../outputs/submission_ensemble_tuned_v9.csv', index=False)\n",
    "print(\"âœ… Submission file saved as '../outputs/submission_ensemble_tuned_v9.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-lab)",
   "language": "python",
   "name": "data-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
