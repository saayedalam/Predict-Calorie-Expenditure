{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b628931f-3492-4538-9d59-1d2fda87bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from init import *  # Adds project root to sys.path\n",
    "from src import config\n",
    "from src.utils import log_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import optuna\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.catboost\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "optuna.logging.set_verbosity(logging.WARNING)\n",
    "optuna.logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420a0224-3c13-4f8e-941e-de99b5f563f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_pickle(config.DATA_PATH + \"processed/X_train_fe.pkl\")\n",
    "test = pd.read_pickle(config.DATA_PATH + \"processed/X_test_fe.pkl\")\n",
    "sample = pd.read_csv(config.DATA_PATH + 'raw/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0dd1a9-57ea-4ece-83fa-2888248b9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preaparting only with Best Features from previous notebook\n",
    "# 1. Define the exact features you want\n",
    "selected_features = [\n",
    "    'Sex',\n",
    "    'Age',\n",
    "    'Height',\n",
    "    'Weight',\n",
    "    'Duration',\n",
    "    'Heart_Rate',\n",
    "    'HR_per_min',\n",
    "    'Age_Group_Adult',\n",
    "    'Age_Group_Senior'\n",
    "]\n",
    "\n",
    "# 2. Extract X and y\n",
    "X = train[selected_features].copy()\n",
    "y = train[\"Calories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c573bfc3-4079-4bd2-8ea4-4acac2cb4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac220ea-7c2c-4d86-b48f-42ee7207e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 18:42:01 INFO mlflow.tracking.fluent: Experiment with name 'Calories - Optuna Tuning' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Running Optuna for RF...\n"
     ]
    }
   ],
   "source": [
    "# Set MLflow location and experiment\n",
    "mlflow.set_tracking_uri(\"file:../logs/mlruns\")\n",
    "mlflow.set_experiment(\"Calories - Optuna Tuning\")\n",
    "\n",
    "# Ensure directory for saving best_params\n",
    "Path(\"../logs/best_params\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define model configs\n",
    "model_configs = {\n",
    "    \"rf\": {\n",
    "        \"class\": RandomForestRegressor,\n",
    "        \"params\": lambda t: {\n",
    "            \"n_estimators\": t.suggest_int('n_estimators', 50, 300),\n",
    "            \"max_depth\": t.suggest_int('max_depth', 3, 15),\n",
    "            \"max_features\": t.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1\n",
    "        }\n",
    "    },\n",
    "    \"hgb\": {\n",
    "        \"class\": HistGradientBoostingRegressor,\n",
    "        \"params\": lambda t: {\n",
    "            \"learning_rate\": t.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            \"max_depth\": t.suggest_int('max_depth', 3, 10),\n",
    "            \"max_iter\": t.suggest_int('max_iter', 50, 200),\n",
    "            \"l2_regularization\": t.suggest_float('l2_regularization', 0.0, 1.0),\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    },\n",
    "    \"xgb\": {\n",
    "        \"class\": XGBRegressor,\n",
    "        \"params\": lambda t: {\n",
    "            \"n_estimators\": t.suggest_int('n_estimators', 50, 200),\n",
    "            \"max_depth\": t.suggest_int('max_depth', 3, 10),\n",
    "            \"learning_rate\": t.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            \"subsample\": t.suggest_float('subsample', 0.6, 1.0),\n",
    "            \"colsample_bytree\": t.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            \"gamma\": t.suggest_float('gamma', 0, 5),\n",
    "            \"reg_alpha\": t.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "            \"reg_lambda\": t.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1\n",
    "        }\n",
    "    },\n",
    "    \"lgbm\": {\n",
    "        \"class\": LGBMRegressor,\n",
    "        \"params\": lambda t: {\n",
    "            \"n_estimators\": t.suggest_int('n_estimators', 50, 300),\n",
    "            \"max_depth\": t.suggest_int('max_depth', 3, 15),\n",
    "            \"learning_rate\": t.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            \"num_leaves\": t.suggest_int('num_leaves', 20, 100),\n",
    "            \"subsample\": t.suggest_float('subsample', 0.6, 1.0),\n",
    "            \"colsample_bytree\": t.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            \"reg_alpha\": t.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "            \"reg_lambda\": t.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "            \"random_state\": 42,\n",
    "            \"verbosity\": -1,\n",
    "            \"n_jobs\": -1\n",
    "        }\n",
    "    },\n",
    "    \"cat\": {\n",
    "        \"class\": CatBoostRegressor,\n",
    "        \"params\": lambda t: {\n",
    "            \"iterations\": t.suggest_int(\"iterations\", 100, 500),\n",
    "            \"depth\": t.suggest_int(\"depth\", 4, 10),\n",
    "            \"learning_rate\": t.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"l2_leaf_reg\": t.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "            \"random_seed\": 42,\n",
    "            \"verbose\": 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def make_objective(model_class, param_func):\n",
    "    def objective(trial):\n",
    "        model = model_class(**param_func(trial))\n",
    "        score = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=3).mean()\n",
    "        return -score\n",
    "    return objective\n",
    "\n",
    "# Run Optuna tuning, log to MLflow, save best_params\n",
    "studies = {}\n",
    "for name, config in model_configs.items():\n",
    "    print(f\"âœ… Running Optuna for {name.upper()}...\")\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Tuning_{name.upper()}\"):\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(make_objective(config[\"class\"], config[\"params\"]), n_trials=100)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_score = study.best_value\n",
    "\n",
    "        # Log to MLflow\n",
    "        mlflow.log_params({f\"{name}__{k}\": v for k, v in best_params.items()})\n",
    "        mlflow.log_metric(f\"{name.upper()}_best_score\", best_score)\n",
    "\n",
    "        # Save best_params to JSON\n",
    "        with open(f\"../logs/best_params/{name}.json\", \"w\") as f:\n",
    "            json.dump(best_params, f, indent=2)\n",
    "\n",
    "        print(f\"âœ… {name.upper()} best params saved and logged.\")\n",
    "        studies[name] = study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36bfae-5866-407c-b382-d967294d0520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Set MLflow location and experiment\n",
    "mlflow.set_tracking_uri(\"file:../logs/mlruns\")\n",
    "mlflow.set_experiment(\"Calories - Tuned Models\")\n",
    "Path(\"../logs/best_params\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper Functions\n",
    "def save_params_to_json(name, params):\n",
    "    with open(f\"../logs/best_params/{name}.json\", \"w\") as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "\n",
    "def train_log_model(name, model, params, pred_val, score, mlflow_module):\n",
    "    mlflow.log_params({f\"{name}__{k}\": v for k, v in params.items()})\n",
    "    mlflow.log_metric(f\"RMSLE_{name.upper()}\", score)\n",
    "    signature = infer_signature(X_val, pred_val)\n",
    "    mlflow_module.log_model(model, artifact_path=f\"{name}_model\", signature=signature, input_example=X_val.iloc[:1])\n",
    "    save_params_to_json(name, params)\n",
    "\n",
    "# Model config\n",
    "model_configs = [\n",
    "    (\"rf\", RandomForestRegressor, rf_study.best_params, mlflow.sklearn),\n",
    "    (\"hgb\", HistGradientBoostingRegressor, hgb_study.best_params, mlflow.sklearn),\n",
    "    (\"xgb\", XGBRegressor, xgb_study.best_params, mlflow.xgboost),\n",
    "    (\"lgbm\", LGBMRegressor, lgbm_study.best_params, mlflow.lightgbm),\n",
    "    (\"cat\", CatBoostRegressor, cat_study.best_params, mlflow.catboost)\n",
    "]\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"tuned_models_all_5\"):\n",
    "    scores = {}\n",
    "\n",
    "    for name, model_cls, params, mlflow_module in model_configs:\n",
    "        kwargs = dict(random_state=42) if name != \"cat\" else dict(random_seed=42, verbose=0, train_dir=\"../logs/catboost_logs\")\n",
    "        kwargs.update(params)\n",
    "        model = model_cls(**kwargs)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "        score = rmsle(y_val, val_pred)\n",
    "        scores[name.upper()] = score\n",
    "\n",
    "        train_log_model(name, model, params, val_pred, score, mlflow_module)\n",
    "\n",
    "    # Print RMSLE Scores\n",
    "    print(\"\\nâœ… RMSLE Scores:\")\n",
    "    for model, score in scores.items():\n",
    "        print(f\"RMSLE {model:5s}: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c0145-fe5f-47b6-863d-fa6434228924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Scores\n",
    "scores = {\n",
    "    \"Random Forest Tuned\": score_rf,\n",
    "    \"HistGradientBoosting Tuned\": score_hgb,\n",
    "    \"XGBoost Tuned\": score_xgb,\n",
    "    \"LightGBM Tuned\": score_lgbm,\n",
    "    \"CatBoost Tuned\": score_cat\n",
    "}\n",
    "\n",
    "for name, score in scores.items():\n",
    "    log_score(name, score, f\"Optuna-tuned {name.split()[0]}, best features, 100 trials V2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba5a3b8-48f6-4dea-8d83-76cb5a0d0cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE Avg Ensemble: 0.060154986951392736\n",
      "RMSLE Weighted Ensemble: 0.05993156413706159\n"
     ]
    }
   ],
   "source": [
    "# === Simple average ensemble (5 models) ===\n",
    "val_avg = (val_rf + val_hgb + val_xgb + val_lgbm + val_cat) / 5\n",
    "print(\"RMSLE Avg Ensemble:\", rmsle(y_val, val_avg))\n",
    "\n",
    "# === Weighted average (adjust weights based on model performance if known)\n",
    "val_weighted = (\n",
    "    0.3 * val_xgb +\n",
    "    0.25 * val_rf +\n",
    "    0.25 * val_cat +\n",
    "    0.1 * val_lgbm +\n",
    "    0.1 * val_hgb\n",
    ")\n",
    "print(\"RMSLE Weighted Ensemble:\", rmsle(y_val, val_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f821cb72-ae7a-4645-8d09-dd784acd2909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Running grid search for weighted ensemble...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip if weights don't sum to ~1\u001b[39;00m\n\u001b[32m     15\u001b[39m w_xgb, w_cat, w_rf, w_lgbm, w_hgb = w\n\u001b[32m     17\u001b[39m val_pred = (\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     w_xgb * \u001b[43mval_xgb\u001b[49m +\n\u001b[32m     19\u001b[39m     w_cat * val_cat +\n\u001b[32m     20\u001b[39m     w_rf * val_rf +\n\u001b[32m     21\u001b[39m     w_lgbm * val_lgbm +\n\u001b[32m     22\u001b[39m     w_hgb * val_hgb\n\u001b[32m     23\u001b[39m )\n\u001b[32m     25\u001b[39m score = rmsle(y_val, val_pred)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m score < best_score:\n",
      "\u001b[31mNameError\u001b[39m: name 'val_xgb' is not defined"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define possible weight values (must sum to 1)\n",
    "weight_range = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "best_score = float('inf')\n",
    "best_weights = None\n",
    "\n",
    "print(\"ðŸ” Running grid search for weighted ensemble...\\n\")\n",
    "\n",
    "for w in product(weight_range, repeat=5):\n",
    "    if abs(sum(w) - 1.0) > 0.001:\n",
    "        continue  # Skip if weights don't sum to ~1\n",
    "\n",
    "    w_xgb, w_cat, w_rf, w_lgbm, w_hgb = w\n",
    "\n",
    "    val_pred = (\n",
    "        w_xgb * val_xgb +\n",
    "        w_cat * val_cat +\n",
    "        w_rf * val_rf +\n",
    "        w_lgbm * val_lgbm +\n",
    "        w_hgb * val_hgb\n",
    "    )\n",
    "\n",
    "    score = rmsle(y_val, val_pred)\n",
    "\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_weights = w\n",
    "\n",
    "        print(f\"âœ… New best RMSLE: {score:.6f} with weights: XGB={w_xgb}, CAT={w_cat}, RF={w_rf}, LGBM={w_lgbm}, HGB={w_hgb}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Best weights found: {best_weights} â†’ RMSLE: {best_score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb6ee9a-ce8a-4b77-9bcf-df2813fac2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"âœ… Model tuning is complete\" | mail -s \"ML Job Done\" 4377791620@fido.ca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c74e58f-a2b4-46e0-b793-6e1d7bfd1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"Test email\" | mail -s \"Testing\" saayedalam@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f2d09-adbd-402e-a2c7-2264b9295b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-lab)",
   "language": "python",
   "name": "data-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
