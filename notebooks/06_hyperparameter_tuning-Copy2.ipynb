{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b628931f-3492-4538-9d59-1d2fda87bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from init import *  # Adds project root to sys.path\n",
    "from src import config\n",
    "from src.utils import log_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import optuna\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.catboost\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "optuna.logging.set_verbosity(logging.WARNING)\n",
    "optuna.logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420a0224-3c13-4f8e-941e-de99b5f563f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_pickle(config.DATA_PATH + \"processed/X_train_fe.pkl\")\n",
    "test = pd.read_pickle(config.DATA_PATH + \"processed/X_test_fe.pkl\")\n",
    "sample = pd.read_csv(config.DATA_PATH + 'raw/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0dd1a9-57ea-4ece-83fa-2888248b9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preaparting only with Best Features from previous notebook\n",
    "# 1. Define the exact features you want\n",
    "selected_features = [\n",
    "    'Sex',\n",
    "    'Age',\n",
    "    'Height',\n",
    "    'Weight',\n",
    "    'Duration',\n",
    "    'Heart_Rate',\n",
    "    'HR_per_min',\n",
    "    'Age_Group_Adult',\n",
    "    'Age_Group_Senior'\n",
    "]\n",
    "\n",
    "# 2. Extract X and y\n",
    "X = train[selected_features].copy()\n",
    "y = train[\"Calories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c573bfc3-4079-4bd2-8ea4-4acac2cb4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac220ea-7c2c-4d86-b48f-42ee7207e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning - Optuna\n",
    "\n",
    "# Random Forest\n",
    "def rf_objective(trial):\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 15),\n",
    "        max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    score = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=3).mean()\n",
    "    return -score\n",
    "\n",
    "# HistGradientBoosting\n",
    "def hgb_objective(trial):\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "        max_iter=trial.suggest_int('max_iter', 50, 200),\n",
    "        l2_regularization=trial.suggest_float('l2_regularization', 0.0, 1.0),\n",
    "        random_state=42\n",
    "    )\n",
    "    score = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=3).mean()\n",
    "    return -score\n",
    "\n",
    "# XGBoost\n",
    "def xgb_objective(trial):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        gamma=trial.suggest_float('gamma', 0, 5),\n",
    "        reg_alpha=trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        reg_lambda=trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    score = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=3).mean()\n",
    "    return -score\n",
    "\n",
    "# LightGBM\n",
    "def lgbm_objective(trial):\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 15),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        num_leaves=trial.suggest_int('num_leaves', 20, 100),\n",
    "        subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        reg_alpha=trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        reg_lambda=trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        random_state=42,\n",
    "        verbosity=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    score = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=3).mean()\n",
    "    return -score\n",
    "\n",
    "# CatBoost\n",
    "def catboost_objective(trial):\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=trial.suggest_int(\"iterations\", 100, 500),\n",
    "        depth=trial.suggest_int(\"depth\", 4, 10),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    score = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=3).mean()\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b90723d-5fad-4363-8b31-d8fb09babffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Running Optuna for RandomForest...\n",
      "Best RF Params: {'n_estimators': 291, 'max_depth': 15, 'max_features': 'log2'}\n",
      "✅ Running Optuna for HGB...\n",
      "Best HGB Params: {'learning_rate': 0.130994262143399, 'max_depth': 10, 'max_iter': 198, 'l2_regularization': 0.7023407953668039}\n",
      "✅ Running Optuna for XGB...\n",
      "Best XGB Params: {'n_estimators': 195, 'max_depth': 10, 'learning_rate': 0.03990489755046095, 'subsample': 0.6270828536610373, 'colsample_bytree': 0.6832104546238639, 'gamma': 1.0687895750485235, 'reg_alpha': 0.04657777019499847, 'reg_lambda': 0.020519254830031176}\n",
      "✅ Running Optuna for LGBM...\n",
      "Best LGBM Params: {'n_estimators': 255, 'max_depth': 8, 'learning_rate': 0.0513321899000855, 'num_leaves': 98, 'subsample': 0.7888096900485683, 'colsample_bytree': 0.6183319373406093, 'reg_alpha': 0.5941258351285894, 'reg_lambda': 0.3016808668191065}\n",
      "✅ Running Optuna for CatBoost...\n",
      "Best CatBoost Params: {'iterations': 462, 'depth': 10, 'learning_rate': 0.08393764522637687, 'l2_leaf_reg': 1.4330091874682247}\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna on All Models\n",
    "\n",
    "print(\"✅ Running Optuna for RandomForest...\")\n",
    "rf_study = optuna.create_study(direction='minimize')\n",
    "rf_study.optimize(rf_objective, n_trials=100)\n",
    "print(\"Best RF Params:\", rf_study.best_params)\n",
    "\n",
    "print(\"✅ Running Optuna for HGB...\")\n",
    "hgb_study = optuna.create_study(direction='minimize')\n",
    "hgb_study.optimize(hgb_objective, n_trials=100)\n",
    "print(\"Best HGB Params:\", hgb_study.best_params)\n",
    "\n",
    "print(\"✅ Running Optuna for XGB...\")\n",
    "xgb_study = optuna.create_study(direction='minimize')\n",
    "xgb_study.optimize(xgb_objective, n_trials=100)\n",
    "print(\"Best XGB Params:\", xgb_study.best_params)\n",
    "\n",
    "print(\"✅ Running Optuna for LGBM...\")\n",
    "lgbm_study = optuna.create_study(direction='minimize')\n",
    "lgbm_study.optimize(lgbm_objective, n_trials=100)\n",
    "print(\"Best LGBM Params:\", lgbm_study.best_params)\n",
    "\n",
    "print(\"✅ Running Optuna for CatBoost...\")\n",
    "cat_study = optuna.create_study(direction='minimize')\n",
    "cat_study.optimize(catboost_objective, n_trials=100)\n",
    "print(\"Best CatBoost Params:\", cat_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d36bfae-5866-407c-b382-d967294d0520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ RMSLE Scores:\n",
      "RMSLE RF   : 0.06175\n",
      "RMSLE HGB  : 0.06453\n",
      "RMSLE XGB  : 0.06053\n",
      "RMSLE LGBM : 0.06202\n",
      "RMSLE CAT  : 0.06094\n"
     ]
    }
   ],
   "source": [
    "# Set MLflow location and experiment\n",
    "mlflow.set_tracking_uri(\"file:../logs/mlruns\")\n",
    "mlflow.set_experiment(\"Calories - Tuned Models\")\n",
    "\n",
    "# Helper function\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, np.clip(y_pred, 0, None)))\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"tuned_models_all_5\"):\n",
    "    scores = {}\n",
    "\n",
    "    # 1. Random Forest\n",
    "    rf = RandomForestRegressor(**rf_study.best_params, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    val_rf = rf.predict(X_val)\n",
    "    score_rf = rmsle(y_val, val_rf)\n",
    "    scores[\"RF\"] = score_rf\n",
    "    mlflow.log_params({f\"rf__{k}\": v for k, v in rf_study.best_params.items()})\n",
    "    mlflow.log_metric(\"RMSLE_RF\", score_rf)\n",
    "    signature_rf = infer_signature(X_val, val_rf)\n",
    "    mlflow.sklearn.log_model(rf, artifact_path=\"rf_model\", signature=signature_rf, input_example=X_val.iloc[:1])\n",
    "\n",
    "    # 2. HGB\n",
    "    hgb = HistGradientBoostingRegressor(**hgb_study.best_params, random_state=42)\n",
    "    hgb.fit(X_train, y_train)\n",
    "    val_hgb = hgb.predict(X_val)\n",
    "    score_hgb = rmsle(y_val, val_hgb)\n",
    "    scores[\"HGB\"] = score_hgb\n",
    "    mlflow.log_params({f\"hgb__{k}\": v for k, v in hgb_study.best_params.items()})\n",
    "    mlflow.log_metric(\"RMSLE_HGB\", score_hgb)\n",
    "    signature_hgb = infer_signature(X_val, val_hgb)\n",
    "    mlflow.sklearn.log_model(hgb, artifact_path=\"hgb_model\", signature=signature_hgb, input_example=X_val.iloc[:1])\n",
    "\n",
    "    # 3. XGBoost\n",
    "    xgb = XGBRegressor(**xgb_study.best_params, random_state=42, verbosity=0)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    val_xgb = xgb.predict(X_val)\n",
    "    score_xgb = rmsle(y_val, val_xgb)\n",
    "    scores[\"XGB\"] = score_xgb\n",
    "    mlflow.log_params({f\"xgb__{k}\": v for k, v in xgb_study.best_params.items()})\n",
    "    mlflow.log_metric(\"RMSLE_XGB\", score_xgb)\n",
    "    signature_xgb = infer_signature(X_val, val_xgb)\n",
    "    mlflow.xgboost.log_model(xgb, artifact_path=\"xgb_model\", signature=signature_xgb, input_example=X_val.iloc[:1])\n",
    "\n",
    "    # 4. LightGBM\n",
    "    lgbm = LGBMRegressor(**lgbm_study.best_params, random_state=42, verbosity=-1)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    val_lgbm = lgbm.predict(X_val)\n",
    "    score_lgbm = rmsle(y_val, val_lgbm)\n",
    "    scores[\"LGBM\"] = score_lgbm\n",
    "    mlflow.log_params({f\"lgbm__{k}\": v for k, v in lgbm_study.best_params.items()})\n",
    "    mlflow.log_metric(\"RMSLE_LGBM\", score_lgbm)\n",
    "    signature_lgbm = infer_signature(X_val, val_lgbm)\n",
    "    mlflow.lightgbm.log_model(lgbm, artifact_path=\"lgbm_model\", signature=signature_lgbm, input_example=X_val.iloc[:1])\n",
    "\n",
    "    # 5. CatBoost\n",
    "    cat = CatBoostRegressor(**cat_study.best_params, random_seed=42, verbose=0, train_dir=\"../logs/catboost_logs\")\n",
    "    cat.fit(X_train, y_train)\n",
    "    val_cat = cat.predict(X_val)\n",
    "    score_cat = rmsle(y_val, val_cat)\n",
    "    scores[\"CAT\"] = score_cat\n",
    "    mlflow.log_params({f\"cat__{k}\": v for k, v in cat_study.best_params.items()})\n",
    "    mlflow.log_metric(\"RMSLE_CAT\", score_cat)\n",
    "    signature_cat = infer_signature(X_val, val_cat)\n",
    "    mlflow.catboost.log_model(cat, artifact_path=\"cat_model\", signature=signature_cat, input_example=X_val.iloc[:1])\n",
    "\n",
    "    # Print scores\n",
    "    print(\"\\n✅ RMSLE Scores:\")\n",
    "    for model, score in scores.items():\n",
    "        print(f\"RMSLE {model:5s}: {score:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "339c0145-fe5f-47b6-863d-fa6434228924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged: Random Forest Tuned | Score: 0.06175\n",
      "✅ Logged: HistGradientBoosting Tuned | Score: 0.06453\n",
      "✅ Logged: XGBoost Tuned | Score: 0.06053\n",
      "✅ Logged: LightGBM Tuned | Score: 0.06202\n",
      "✅ Logged: CatBoost Tuned | Score: 0.06094\n"
     ]
    }
   ],
   "source": [
    "# Log Scores\n",
    "log_score(\"Random Forest Tuned\", score_rf, \"Optuna-tuned RF, best features, 100 trials\")\n",
    "log_score(\"HistGradientBoosting Tuned\", score_hgb, \"Optuna-tuned HGB, best features, 100 trials\")\n",
    "log_score(\"XGBoost Tuned\", score_xgb, \"Optuna-tuned XGB, best features, 100 trials\")\n",
    "log_score(\"LightGBM Tuned\", score_lgbm, \"Optuna-tuned LGBM, best features, 100 trials\")\n",
    "log_score(\"CatBoost Tuned\", score_cat, \"Optuna-tuned CatBoost, best features, 100 trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba5a3b8-48f6-4dea-8d83-76cb5a0d0cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE Avg Ensemble: 0.060154986951392736\n",
      "RMSLE Weighted Ensemble: 0.05993156413706159\n"
     ]
    }
   ],
   "source": [
    "# === Simple average ensemble (5 models) ===\n",
    "val_avg = (val_rf + val_hgb + val_xgb + val_lgbm + val_cat) / 5\n",
    "print(\"RMSLE Avg Ensemble:\", rmsle(y_val, val_avg))\n",
    "\n",
    "# === Weighted average (adjust weights based on model performance if known)\n",
    "val_weighted = (\n",
    "    0.3 * val_xgb +\n",
    "    0.25 * val_rf +\n",
    "    0.25 * val_cat +\n",
    "    0.1 * val_lgbm +\n",
    "    0.1 * val_hgb\n",
    ")\n",
    "print(\"RMSLE Weighted Ensemble:\", rmsle(y_val, val_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821cb72-ae7a-4645-8d09-dd784acd2909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-lab)",
   "language": "python",
   "name": "data-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
